python -m torch.distributed.launch --nproc_per_node 2 train.py
nohup python -m torch.distributed.launch --nproc_per_node 2 train.py > train.log 2>&1 &
python test.py
